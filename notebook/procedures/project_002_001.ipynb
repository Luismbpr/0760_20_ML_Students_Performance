{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d30c30d4",
   "metadata": {},
   "source": [
    "### EDA\n",
    "EDA Goes into further detail\n",
    "* notebook/Project_EDA_001_001.ipynb\n",
    "* notebook/Project_EDA_002_001.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3192fcb9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65e56508",
   "metadata": {},
   "source": [
    "### Notebook with model training\n",
    "\n",
    "notebook/Project_Model_Training.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3359cb92",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbd3bf19",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2291e03",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Inside data_ingestion.py - src/components/data_ingestion.py\n",
    "\n",
    "```python\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from src.exception import CustomException\n",
    "from src.logger import logging\n",
    "\n",
    "from src.components.data_transformation import DataTransformation\n",
    "\n",
    "@dataclass\n",
    "class DataIngestionConfig:\n",
    "    train_data_path: str = os.path.join('artifacts',\"train.csv\")\n",
    "    test_data_path: str = os.path.join('artifacts',\"test.csv\")\n",
    "    raw_data_path: str = os.path.join('artifacts', 'data.csv')\n",
    "\n",
    "class DataIngestion:\n",
    "    def __init__(self):\n",
    "        self.ingestion_config = DataIngestionConfig()\n",
    "    \n",
    "    def initiate_data_ingestion(self):\n",
    "        logging.info(\"Entered the data ingestion method or component\")\n",
    "        try:\n",
    "            #pass\n",
    "            df = pd.read_csv(\"notebooks/data/students_performance_0760.csv\")\n",
    "            logging.info(\"Reading the dataset as a DataFrame\")\n",
    "            \n",
    "            os.makedirs(os.path.dirname(self.ingestion_config.train_data_path), exist_ok=True)\n",
    "\n",
    "            df.to_csv(self.ingestion_config.raw_data_path, index=False, header=True)\n",
    "\n",
    "            logging.info(\"Initiating Train Test Split\")\n",
    "            train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)\n",
    "            logging.info(\"Train Test Split Complete\")\n",
    "            \n",
    "            train_set.to_csv(self.ingestion_config.train_data_path, index=False, header=True)\n",
    "            test_set.to_csv(self.ingestion_config.test_data_path, index=False, header=True)\n",
    "            #logging.info(\"Train and Test Set Saved\")\n",
    "            logging.info(\"Data Ingestion Complete\")\n",
    "            return(\n",
    "                self.ingestion_config.train_data_path,\n",
    "                self.ingestion_config.test_data_path\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    obj = DataIngestion()\n",
    "    train_data, test_data = obj.initiate_data_ingestion()\n",
    "```\n",
    "\n",
    "Testing it\n",
    "\n",
    "```Shell\n",
    "% python src/components/data_ingestion.py\n",
    "```\n",
    "\n",
    "Inside logs\n",
    "```Markdown\n",
    "logs/04_07_2025_16_13_09.log/04_07_2025_16_13_09.log\n",
    "[ 2025-04-07 16:13:09,068 ] 22 root - INFO - Entered the data ingestion method or component\n",
    "[ 2025-04-07 16:13:09,074 ] 26 root - INFO - Reading the dataset as a DataFrame\n",
    "[ 2025-04-07 16:13:09,076 ] 32 root - INFO - Initiating Train Test Split\n",
    "[ 2025-04-07 16:13:09,077 ] 34 root - INFO - Train Test Split Completed\n",
    "[ 2025-04-07 16:13:09,079 ] 38 root - INFO - Train and Test Set Saved\n",
    "[ 2025-04-07 16:13:09,079 ] 39 root - INFO - Data Ingestion Completed\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d524d5b9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed30ab12",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Inside - src/utils.py\n",
    "\n",
    "```python\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from src.exception import CustomException\n",
    "from src.logger import logging\n",
    "\n",
    "\n",
    "def save_object(file_path, obj):\n",
    "    \"\"\"\n",
    "    Saves object. Saves pickle file object\n",
    "    from src.utils import save_object\n",
    "    save_object(file_path=, obj=)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dir_path = os.path.dirname(file_path)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        \n",
    "        with os.open(file_path, \"wb\") as file_obj:\n",
    "            pickle.dump(obj, file_obj)\n",
    "    except Exception as e:\n",
    "        raise CustomException(e,sys)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde895a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Inside - src/components/data_transformation.py\n",
    "\n",
    "```Python\n",
    "import sys\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from src.logger import logging\n",
    "from src.exception import CustomException\n",
    "from src.utils import save_object\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    preprocessor_obj_file_path:str=os.path.join('artifacts','preprocessor.pkl')\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self):\n",
    "        self.data_transformation_config=DataTransformationConfig\n",
    "    \n",
    "    def get_data_transformer_object(self):\n",
    "        \"\"\"\n",
    "        This function Created the data transformation pipeline for all the columns.\n",
    "\n",
    "        Numerical Pipeline\n",
    "        Selects and transforms the Numerical columns\n",
    "            SimpleImputer - Handles the missing values\n",
    "            Performs Standard Scaling\n",
    "\n",
    "        Categorical Pipeline\n",
    "        Selects and transforms the Categorical columns\n",
    "            SimpleImputer - Handles the missing values\n",
    "            Performs OneHotEncoding\n",
    "            Performs Standard Scaling\n",
    "\n",
    "        Args\n",
    "          .\n",
    "          \n",
    "        Returns\n",
    "          preprocessor\n",
    "        \"\"\"\n",
    "        try:\n",
    "            #pass\n",
    "            numerical_columns= [\n",
    "                \"reading_score\",\n",
    "                \"writing_score\"\n",
    "            ]\n",
    "            categorical_columns=[\n",
    "                \"gender\",\n",
    "                \"race_ethnicity\",\n",
    "                \"parental_level_of_education\",\n",
    "                \"lunch\",\n",
    "                \"test_preparation_course\"\n",
    "            ]\n",
    "\n",
    "            ## Pipelines\n",
    "            ## SimpleImputer - Handles the missing values\n",
    "            ## Numerical Pipeline\n",
    "            num_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                    (\"scaler\", StandardScaler())\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            ## Categorical Pipeline\n",
    "            cat_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                    (\"one_hot_encoder\", OneHotEncoder()),\n",
    "                    (\"scaler\", StandardScaler(with_mean=False))\n",
    "                ]\n",
    "            )\n",
    "            logging.info(f\"Numerical columns: {numerical_columns}\")\n",
    "            logging.info(f\"Categorical columns: {categorical_columns}\")\n",
    "            \n",
    "            ## Merging both pipelines\n",
    "            preprocessor = ColumnTransformer([\n",
    "                (\"num_pipeline\", num_pipeline, numerical_columns),\n",
    "                (\"cat_pipeline\", cat_pipeline, categorical_columns)\n",
    "                ]\n",
    "            )\n",
    "            return preprocessor\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "    \n",
    "\n",
    "    def initiate_data_transformation(self, train_path, test_path):\n",
    "        \"\"\"\n",
    "        This function performs data transformation\n",
    "        Args\n",
    "          train_path\n",
    "          test_path\n",
    "        \n",
    "        Returns\n",
    "          train_arr\n",
    "          test_arr\n",
    "          self.data_transformation_config.preprocessor_obj_file_path\n",
    "        \n",
    "        from src.components.data_transformation import initiate_data_transformation\n",
    "\n",
    "        initiate_data_transformation(train_path=, test_path=)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            #pass\n",
    "            train_df = pd.read_csv(train_path)\n",
    "            test_df = pd.read_csv(test_path)\n",
    "            logging.info(\"Completed Reading Train and Test Data\")\n",
    "            logging.info(\"Obtaining Preprocessing Object\")\n",
    "            preprocessing_obj = self.get_data_transformer_object()\n",
    "\n",
    "            target_column_name = \"math_score\"\n",
    "            #numerical_columns= [\"reading_score\",\"writing_score\"]\n",
    "            #categorical_columns=[\"gender\",\n",
    "                                 #\"race_ethnicity\",\n",
    "                                 #\"parental_level_of_education\",\n",
    "                                 #\"lunch\",\n",
    "                                 #\"test_preparation_course\"\n",
    "                                 #]\n",
    "            \n",
    "            ## Selecting X and y\n",
    "            input_feature_train_df = train_df.drop(columns=[target_column_name], axis=1)\n",
    "            target_feature_train_df = train_df[target_column_name]\n",
    "\n",
    "            input_feature_test_df = test_df.drop(columns=[target_column_name], axis=1)\n",
    "            target_feature_test_df = test_df[target_column_name]\n",
    "\n",
    "            logging.info(f\"Applying preprocessing object on the training and testing dataframes.\")\n",
    "\n",
    "            ## Fit Transform Train - Transform Test\n",
    "            input_feature_train_arr = preprocessing_obj.fit_transform(input_feature_train_df)\n",
    "            input_feature_test_arr = preprocessing_obj.transform(input_feature_test_df)\n",
    "\n",
    "            ## Concat Transformed data with target feature\n",
    "            train_arr = np.c_[\n",
    "                input_feature_train_arr, np.array(target_feature_train_df)\n",
    "            ]\n",
    "            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]\n",
    "            \n",
    "            ## Create/get function in utils/save_object()\n",
    "            save_object(\n",
    "                file_path=self.data_transformation_config.preprocessor_obj_file_path,\n",
    "                obj=preprocessing_obj\n",
    "            )\n",
    "            \n",
    "            logging.info(\"Preprocessing Object Saved\")\n",
    "\n",
    "            return (\n",
    "                train_arr,\n",
    "                test_arr,\n",
    "                self.data_transformation_config.preprocessor_obj_file_path,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79202c2c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "For testing this need to add the code into data_ingestion.py - src/components/data_ingestion.py\n",
    "\n",
    "```python\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from src.exception import CustomException\n",
    "from src.logger import logging\n",
    "\n",
    "from src.components.data_transformation import DataTransformation\n",
    "\n",
    "@dataclass\n",
    "class DataIngestionConfig:\n",
    "    train_data_path: str = os.path.join('artifacts',\"train.csv\")\n",
    "    test_data_path: str = os.path.join('artifacts',\"test.csv\")\n",
    "    raw_data_path: str = os.path.join('artifacts', 'data.csv')\n",
    "\n",
    "class DataIngestion:\n",
    "    def __init__(self):\n",
    "        self.ingestion_config = DataIngestionConfig()\n",
    "    \n",
    "    def initiate_data_ingestion(self):\n",
    "        logging.info(\"Entered the data ingestion method or component\")\n",
    "        try:\n",
    "            #pass\n",
    "            df = pd.read_csv(\"notebooks/data/students_performance_0760.csv\")\n",
    "            logging.info(\"Reading the dataset as a DataFrame\")\n",
    "            \n",
    "            os.makedirs(os.path.dirname(self.ingestion_config.train_data_path), exist_ok=True)\n",
    "\n",
    "            df.to_csv(self.ingestion_config.raw_data_path, index=False, header=True)\n",
    "\n",
    "            logging.info(\"Initiating Train Test Split\")\n",
    "            train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)\n",
    "            logging.info(\"Train Test Split Complete\")\n",
    "            \n",
    "            train_set.to_csv(self.ingestion_config.train_data_path, index=False, header=True)\n",
    "            test_set.to_csv(self.ingestion_config.test_data_path, index=False, header=True)\n",
    "            #logging.info(\"Train and Test Set Saved\")\n",
    "            logging.info(\"Data Ingestion Complete\")\n",
    "            return(\n",
    "                self.ingestion_config.train_data_path,\n",
    "                self.ingestion_config.test_data_path\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    obj = DataIngestion()\n",
    "    train_data, test_data = obj.initiate_data_ingestion()\n",
    "\n",
    "    data_transformation = DataTransformation()\n",
    "    train_arr, test_arr,_ = data_transformation.initiate_data_transformation(train_path=train_data, test_path=test_data)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Testing it\n",
    "\n",
    "```Shell\n",
    "% python src/components/data_ingestion.py\n",
    "```\n",
    "\n",
    "Inside logs - logs/.../(...).log\n",
    "```Markdown\n",
    "[ 2025-04-08 15:13:42,020 ] 24 root - INFO - Entered the data ingestion method or component\n",
    "[ 2025-04-08 15:13:42,028 ] 28 root - INFO - Reading the dataset as a DataFrame\n",
    "[ 2025-04-08 15:13:42,031 ] 34 root - INFO - Initiating Train Test Split\n",
    "[ 2025-04-08 15:13:42,032 ] 36 root - INFO - Train Test Split Complete\n",
    "[ 2025-04-08 15:13:42,034 ] 41 root - INFO - Data Ingestion Complete\n",
    "[ 2025-04-08 15:13:42,036 ] 114 root - INFO - Completed Reading Train and Test Data\n",
    "[ 2025-04-08 15:13:42,036 ] 115 root - INFO - Obtaining Preprocessing Object\n",
    "[ 2025-04-08 15:13:42,036 ] 79 root - INFO - Numerical columns: ['reading_score', 'writing_score']\n",
    "[ 2025-04-08 15:13:42,036 ] 80 root - INFO - Categorical columns: ['gender', 'race_ethnicity', 'parental_level_of_education', 'lunch', 'test_preparation_course']\n",
    "[ 2025-04-08 15:13:42,036 ] 134 root - INFO - Applying preprocessing object on the training and testing dataframes.\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
